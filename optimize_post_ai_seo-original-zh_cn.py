import json
import re
from bs4 import BeautifulSoup
from openai import OpenAI

def extract_and_optimize_html(html_content):
    soup = BeautifulSoup(html_content, 'html.parser')
    
    # ç²¾ç¡®æå– product-main ä¸»è¦å†…å®¹
    product_main = soup.select_one('div.product-main') # ä½¿ç”¨ CSS é€‰æ‹©å™¨ç²¾ç¡®æå– product-main,æ–‡ç« çš„ä¸»è¦å†…å®¹div
    if not product_main:
        raise ValueError("âŒ No <div class='product-main'> found in the HTML")

    # ä½¿ç”¨CSSé€‰æ‹©å™¨æ’é™¤product-contentã€product-divåŠå…¶å­å…ƒç´ 

    main_content = product_main.select(
        '''
        :not(div.product-div):not(div.product-div *):not(div.product-content):not(div.product-content *),
        h2,
        h3,
        p:not(:has(img))
        '''
    )

    
    print("======== å®é™…å¤„ç†å…ƒç´ ç»Ÿè®¡ ========")
    print(f'æ€»å…ƒç´ æ•°: {len(main_content)}')
    print('å…ƒç´ ç±»å‹åˆ†å¸ƒ:', {tag.name: len(list(product_main.find_all(tag.name))) for tag in main_content})
    print('æ ·ä¾‹å†…å®¹:', [str(tag)[:100] for tag in main_content[:3]] if main_content else 'ç©º')

    # æå–ç»“æ„åŒ–å†…å®¹
    extracted_text = {"h2": [], "h3": [], "p": []}
    for tag in main_content:
        if tag.name in ['h2', 'h3']:
            extracted_text[tag.name].append(tag.get_text(strip=True))
        elif tag.name == 'p':
            # ä¿ç•™å›¾ç‰‡ä½†ç§»é™¤å…¶ä»–æ— å…³å…ƒç´ 
            text = ''.join([str(t) for t in tag.contents 
                           if (not hasattr(t, 'name')) 
                           or t.name not in ['script']])
            extracted_text['p'].append(text)

    print("======== æå–çš„æ­£æ–‡å†…å®¹ ========")
    print(extracted_text)  # æ£€æŸ¥æ˜¯å¦çœŸçš„æå–åˆ°æ­£æ–‡

    # å¦‚æœæå–çš„æ­£æ–‡ä¸ºç©ºï¼Œç›´æ¥è¿”å›åŸ HTML
    if not any(extracted_text.values()):
        print("âŒ æå–çš„æ­£æ–‡å†…å®¹ä¸ºç©ºï¼Œå¯èƒ½ HTML è§£ææœ‰é—®é¢˜ï¼")
        return str(soup)

    cleaned_text = json.dumps(extracted_text, ensure_ascii=False, indent=2)

    # ç”ŸæˆåŸå§‹å…ƒç´ æ•°é‡ç»Ÿè®¡
    original_counts = {k: len(v) for k, v in extracted_text.items()}

    # è°ƒç”¨ OpenAI API
    client = OpenAI(
        api_key="# ä½ çš„api_key", 
        base_url="https://api.chatanywhere.tech/v1" # è°ƒç”¨apiçš„åœ°å€
        )
    
    response = client.chat.completions.create(
        model="deepseek-r1",
        messages=[
            {"role": "system", "content": "You are a helpful assistant that optimizes text."},
            {"role": "user", "content": f"ä¸¥æ ¼éµå¾ªä»¥ä¸‹è¦æ±‚ä¼˜åŒ–æ–‡æœ¬:\n1. ä¿æŒåŸå§‹h2/h3/pæ ‡ç­¾çš„æ•°é‡å’Œé¡ºåº,ä¸è¦éšæ„çš„åˆå¹¶æ ‡ç­¾,åªæ˜¯å¯¹æ ‡ç­¾å†…å®¹è¿›è¡Œä¼˜åŒ–\n2. Role: SEO Optimization Specialist | Language: English | Expertise: SEO strategies & best practices | Skills: Technical SEO (audit, schema, sitemaps, speed), Content Optimization (keywords, on-page, quality, internal linking) | Rules: Ethical SEO, transparency, continuous learning, user experience focus | Workflows: Audit, keyword research, content optimization, performance monitoring | Goal: Improve website visibility & organic traffic.\n3. è¿”å›JSONæ ¼å¼: {{'h2': [...], 'h3': [...], 'p': [...]}}\n4. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—\n5. ç¡®ä¿æ¯ä¸ªæ•°ç»„å…ƒç´ æ•°é‡ä¸åŸå§‹æ•°æ®å®Œå…¨ä¸€è‡´\n\nåŸå§‹å†…å®¹ç»“æ„ç»Ÿè®¡:\n{original_counts}\n\nå¾…ä¼˜åŒ–å†…å®¹:\n{cleaned_text}"}
        ], #é‡Œé¢çš„ç¬¬äºŒæ¡æŒ‡ä»¤n2æ˜¯ç»™AIçš„æŒ‡ä»¤,å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹.
        temperature=1,  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„éšæœºæ€§
        max_tokens=4000,  # æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„é•¿åº¦
    )

    # å¼ºåŒ–æ•°æ®æ ¡éªŒé€»è¾‘
    ai_response_text = response.choices[0].message.content.strip()
    print("======== AI è¿”å›çš„åŸå§‹æ•°æ® ========")
    print(ai_response_text)

    try:
        optimized_text = json.loads(ai_response_text)
        
        # ç»“æ„å®Œæ•´æ€§æ ¡éªŒ
        required_keys = ['h2', 'h3', 'p']
        if not all(k in optimized_text for k in required_keys):
            raise ValueError("AIè¿”å›ç¼ºå°‘å¿…è¦å­—æ®µ")
        
        # ç±»å‹æ ¡éªŒ
        for key in required_keys:
            if not isinstance(optimized_text[key], list):
                raise ValueError(f"{key} å­—æ®µç±»å‹é”™è¯¯ï¼Œåº”ä¸ºåˆ—è¡¨")
            
            # æ£€æŸ¥å…ƒç´ ç±»å‹æ˜¯å¦åŒ¹é…
            for item in optimized_text[key]:
                if not isinstance(item, str):
                    raise ValueError(f"{key} åˆ—è¡¨åŒ…å«éå­—ç¬¦ä¸²å…ƒç´ ")

        # æ•°é‡ä¸¥æ ¼åŒ¹é…æ ¡éªŒ
        original_counts = {k: len(v) for k, v in extracted_text.items()}
        optimized_counts = {k: len(v) for k, v in optimized_text.items()}
        
        print("======== æ•°æ®æ ¡éªŒç»“æœ ========")
        print(f"åŸå§‹æ•°é‡: {original_counts}")
        print(f"ä¼˜åŒ–æ•°é‡: {optimized_counts}")
        
        if optimized_counts != original_counts:
            mismatch_details = {k: f"{optimized_counts[k]}/{original_counts[k]}" 
                              for k in original_counts if optimized_counts[k] != original_counts[k]}
            raise ValueError(f"å…ƒç´ æ•°é‡ä¸åŒ¹é…: {mismatch_details}")
            
    except Exception as e:
        print(f"âŒ æ•°æ®æ ¡éªŒå¤±è´¥: {str(e)}")
        with open("error_backup.json", "w", encoding="utf-8") as f:
            json.dump({
                "error": str(e), 
                "original_counts": original_counts,
                "optimized_counts": optimized_counts,
                "response": ai_response_text
            }, f, indent=2)
        print("âš ï¸ å·²ä¿å­˜è¯¦ç»†é”™è¯¯æ—¥å¿—åˆ° error_backup.json")
        return str(soup)

    # ç²¾å‡†æ›¿æ¢å†…å®¹
    # ä½¿ç”¨ç´¢å¼•ä¿è¯é¡ºåºä¸€è‡´æ€§
    replacement_index = {k: 0 for k in ['h2', 'h3', 'p']}
    content_order = [(i, tag.name) for i, tag in enumerate(main_content)]  # è®°å½•åŸå§‹é¡ºåºç´¢å¼•
    
    print("======== å¼€å§‹å†…å®¹æ›¿æ¢ ========")
    for tag in main_content:
        tag_type = tag.name
        if tag_type in ['h2', 'h3', 'p']:
            if tag_type == 'p':
                target_key = 'p'
            else:
                target_key = tag_type
            
            if replacement_index[target_key] >= len(optimized_text[target_key]):
                print(f"âš ï¸ {target_key} æ›¿æ¢ç´¢å¼•è¶Šç•Œï¼Œè·³è¿‡è¯¥æ ‡ç­¾")
                continue
                
            new_text = optimized_text[target_key][replacement_index[target_key]]
            replacement_index[target_key] += 1
            
            # æ‰§è¡Œæ›¿æ¢
            if tag_type in ['h2', 'h3']:
                original = tag.get_text(strip=True)
                print(f"æ›¿æ¢ {tag_type}: {original[:50]}... â‡’ {new_text[:50]}...")
                tag.string = new_text
            else:
                print(f"æ›¿æ¢ pæ ‡ç­¾: åŸé•¿åº¦{len(''.join(tag.strings))} â‡’ æ–°é•¿åº¦{len(new_text)}")
                tag.clear()
                tag.append(BeautifulSoup(new_text, 'html.parser'))

    return str(soup)

# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    import os
    from tkinter import Tk
    from tkinter.filedialog import askdirectory

    # åˆ›å»ºæ–‡ä»¶é€‰æ‹©å¯¹è¯æ¡†
    Tk().withdraw()
    folder_path = askdirectory(title='é€‰æ‹©åŒ…å«HTMLæ–‡ä»¶çš„æ–‡ä»¶å¤¹')
    if not folder_path:
        print("âŒ æœªé€‰æ‹©æ–‡ä»¶å¤¹ï¼Œç¨‹åºé€€å‡º")
        exit()

    # æ·»åŠ è¾“å‡ºç›®å½•é€‰æ‹©
    output_dir = askdirectory(title='é€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå–æ¶ˆå°†ä½¿ç”¨é»˜è®¤ç›®å½•ï¼‰')
    if not output_dir:
        import datetime
        current_time = datetime.datetime.now().strftime('%y%m%d_%H%M')
        output_dir = os.path.join(folder_path, f'optimized_{current_time}')
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è®°å½•é”™è¯¯æ—¥å¿—
    error_log = os.path.join(output_dir, 'error_log.txt')
    processed = 0
    success_count = 0
    error_files = []
    processing_round = 0  # æ·»åŠ å¤„ç†è½®æ¬¡è®¡æ•°å™¨

    while processing_round < 1:  # æ§åˆ¶åªæ‰§è¡Œä¸€è½®
        for root, dirs, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith('.html'):
                    file_path = os.path.join(root, file)
                    try:
                        with open(file_path, "r", encoding="utf-8") as f:
                            html_content = f.read()

                        print(f"\nğŸ” æ­£åœ¨å¤„ç†: {file}")
                        optimized_html = extract_and_optimize_html(html_content)

                        # ç”Ÿæˆè¾“å‡ºè·¯å¾„
                        rel_path = os.path.relpath(root, folder_path)
                        output_path = os.path.join(output_dir, rel_path, 
                                                f"{os.path.splitext(file)[0]}_optimized.html")
                        os.makedirs(os.path.dirname(output_path), exist_ok=True)

                        with open(output_path, "w", encoding="utf-8") as f:
                            f.write(optimized_html)
                        
                        success_count += 1
                        print(f"âœ… æˆåŠŸä¿å­˜åˆ°: {os.path.relpath(output_path, folder_path)}")
                    except Exception as e:
                        print(f"âŒ å¤„ç†å¤±è´¥: {file} - {str(e)}")
                        error_files.append(file_path)
                    
                    processed += 1
                    print(f"è¿›åº¦: {processed}æ–‡ä»¶å¤„ç†å®Œæˆ ({success_count}æˆåŠŸ {len(error_files)}å¤±è´¥)")
        processing_round += 1  # å®Œæˆä¸€è½®å¤„ç†åé€’å¢

    # è¾“å‡ºç»Ÿè®¡æŠ¥å‘Š
    print(f"\n{'='*30}\nå¤„ç†å®Œæˆç»Ÿè®¡:\n"
          f"â€¢ æ€»å¤„ç†æ–‡ä»¶: {processed}\n"
          f"â€¢ æˆåŠŸæ•°é‡: {success_count}\n"
          f"â€¢ å¤±è´¥æ–‡ä»¶: {len(error_files)}\n"
          f"è¾“å‡ºç›®å½•: {output_dir}")

    # è®°å½•é”™è¯¯æ—¥å¿—
    if error_files:
        error_log = os.path.join(output_dir, 'error_log.txt')
        with open(error_log, 'w', encoding='utf-8') as f:
            f.write('\n'.join(error_files))
        print(f"âš ï¸ å¤±è´¥æ–‡ä»¶åˆ—è¡¨å·²ä¿å­˜è‡³: {error_log}")